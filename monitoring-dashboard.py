#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
ì‹¤ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì•Œë¦¼ì„ ë°œì†¡í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import aiohttp
import redis
from elasticsearch import AsyncElasticsearch
from fastapi import FastAPI, WebSocket, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

# ì„¤ì •
ELASTICSEARCH_URL = "http://localhost:9200"
REDIS_URL = "redis://localhost:6379"
LOG_INDEX_PATTERN = "logs-*"

app = FastAPI(title="Log Monitoring Dashboard")
templates = Jinja2Templates(directory="templates")

class LogAlert(BaseModel):
    timestamp: datetime
    level: str
    service: str
    message: str
    environment: str
    alert_type: str

class MonitoringService:
    def __init__(self):
        self.es = AsyncElasticsearch([ELASTICSEARCH_URL])
        self.redis_client = redis.Redis.from_url(REDIS_URL)
        self.active_connections: List[WebSocket] = []

    async def get_log_statistics(self) -> Dict:
        """ë¡œê·¸ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            # ìµœê·¼ 1ì‹œê°„ ë¡œê·¸ í†µê³„
            query = {
                "query": {
                    "range": {
                        "@timestamp": {
                            "gte": "now-1h"
                        }
                    }
                },
                "aggs": {
                    "log_levels": {
                        "terms": {
                            "field": "log_level.keyword",
                            "size": 10
                        }
                    },
                    "services": {
                        "terms": {
                            "field": "service_name.keyword", 
                            "size": 20
                        }
                    },
                    "environments": {
                        "terms": {
                            "field": "environment.keyword",
                            "size": 10
                        }
                    },
                    "error_timeline": {
                        "date_histogram": {
                            "field": "@timestamp",
                            "fixed_interval": "5m"
                        },
                        "aggs": {
                            "error_count": {
                                "filter": {
                                    "terms": {
                                        "log_level.keyword": ["ERROR", "FATAL"]
                                    }
                                }
                            }
                        }
                    }
                }
            }
            
            response = await self.es.search(
                index=LOG_INDEX_PATTERN,
                body=query,
                size=0
            )
            
            return {
                "total_logs": response["hits"]["total"]["value"],
                "log_levels": response["aggregations"]["log_levels"]["buckets"],
                "services": response["aggregations"]["services"]["buckets"],
                "environments": response["aggregations"]["environments"]["buckets"],
                "error_timeline": response["aggregations"]["error_timeline"]["buckets"]
            }
        except Exception as e:
            logging.error(f"í†µê³„ ì¡°íšŒ ì—ëŸ¬: {e}")
            return {}

    async def get_recent_errors(self, limit: int = 50) -> List[Dict]:
        """ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ"""
        try:
            query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "range": {
                                    "@timestamp": {
                                        "gte": "now-15m"
                                    }
                                }
                            },
                            {
                                "terms": {
                                    "log_level.keyword": ["ERROR", "FATAL"]
                                }
                            }
                        ]
                    }
                },
                "sort": [
                    {
                        "@timestamp": {
                            "order": "desc"
                        }
                    }
                ]
            }
            
            response = await self.es.search(
                index=LOG_INDEX_PATTERN,
                body=query,
                size=limit
            )
            
            return [hit["_source"] for hit in response["hits"]["hits"]]
        except Exception as e:
            logging.error(f"ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def detect_anomalies(self) -> List[LogAlert]:
        """ë¡œê·¸ íŒ¨í„´ ì´ìƒ ê°ì§€"""
        alerts = []
        
        try:
            # ì—ëŸ¬ ë¡œê·¸ ê¸‰ì¦ ê°ì§€
            error_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "range": {
                                    "@timestamp": {
                                        "gte": "now-5m"
                                    }
                                }
                            },
                            {
                                "terms": {
                                    "log_level.keyword": ["ERROR", "FATAL"]
                                }
                            }
                        ]
                    }
                },
                "aggs": {
                    "services": {
                        "terms": {
                            "field": "service_name.keyword",
                            "size": 50
                        }
                    }
                }
            }
            
            response = await self.es.search(
                index=LOG_INDEX_PATTERN,
                body=error_query,
                size=0
            )
            
            total_errors = response["hits"]["total"]["value"]
            if total_errors > 20:  # 5ë¶„ê°„ 20ê°œ ì´ìƒ ì—ëŸ¬
                alerts.append(LogAlert(
                    timestamp=datetime.now(),
                    level="critical",
                    service="system",
                    message=f"ì—ëŸ¬ ë¡œê·¸ ê¸‰ì¦ ê°ì§€: {total_errors}ê±´",
                    environment="production",
                    alert_type="error_spike"
                ))
            
            # ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ ì²´í¬
            for bucket in response["aggregations"]["services"]["buckets"]:
                service_name = bucket["key"]
                error_count = bucket["doc_count"]
                
                if error_count > 10:  # ì„œë¹„ìŠ¤ë³„ 10ê°œ ì´ìƒ ì—ëŸ¬
                    alerts.append(LogAlert(
                        timestamp=datetime.now(),
                        level="warning",
                        service=service_name,
                        message=f"ì„œë¹„ìŠ¤ ì—ëŸ¬ ì¦ê°€: {error_count}ê±´",
                        environment="production", 
                        alert_type="service_error"
                    ))
            
            # ì‘ë‹µ ì‹œê°„ ì´ìƒ ê°ì§€
            slow_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "range": {
                                    "@timestamp": {
                                        "gte": "now-5m"
                                    }
                                }
                            },
                            {
                                "range": {
                                    "response_time": {
                                        "gte": 1000
                                    }
                                }
                            }
                        ]
                    }
                }
            }
            
            slow_response = await self.es.search(
                index=LOG_INDEX_PATTERN,
                body=slow_query,
                size=0
            )
            
            slow_count = slow_response["hits"]["total"]["value"]
            if slow_count > 5:
                alerts.append(LogAlert(
                    timestamp=datetime.now(),
                    level="warning",
                    service="performance",
                    message=f"ì‘ë‹µ ì‹œê°„ ì§€ì—°: {slow_count}ê±´",
                    environment="production",
                    alert_type="performance"
                ))
                
        except Exception as e:
            logging.error(f"ì´ìƒ ê°ì§€ ì‹¤íŒ¨: {e}")
        
        return alerts

    async def send_alert(self, alert: LogAlert):
        """ì•Œë¦¼ ë°œì†¡"""
        # Redisì— ì•Œë¦¼ ì €ì¥
        alert_data = alert.dict()
        alert_data['timestamp'] = alert.timestamp.isoformat()
        
        self.redis_client.lpush("alerts", json.dumps(alert_data))
        self.redis_client.ltrim("alerts", 0, 999)  # ìµœê·¼ 1000ê±´ë§Œ ìœ ì§€
        
        # WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ì•Œë¦¼
        if self.active_connections:
            message = {
                "type": "alert",
                "data": alert_data
            }
            await self.broadcast_message(json.dumps(message))
        
        # ì‹¬ê°í•œ ì•Œë¦¼ì€ ì™¸ë¶€ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
        if alert.level == "critical":
            await self.send_external_alert(alert)

    async def send_external_alert(self, alert: LogAlert):
        """ì™¸ë¶€ ì•Œë¦¼ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡ (Slack, Email ë“±)"""
        try:
            # Slack ì›¹í›… ì˜ˆì‹œ
            webhook_url = "YOUR_SLACK_WEBHOOK_URL"
            
            payload = {
                "text": f"ğŸš¨ Critical Alert: {alert.message}",
                "attachments": [
                    {
                        "color": "danger",
                        "fields": [
                            {"title": "Service", "value": alert.service, "short": True},
                            {"title": "Environment", "value": alert.environment, "short": True},
                            {"title": "Time", "value": alert.timestamp.strftime("%Y-%m-%d %H:%M:%S"), "short": True},
                            {"title": "Type", "value": alert.alert_type, "short": True}
                        ]
                    }
                ]
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(webhook_url, json=payload) as response:
                    if response.status != 200:
                        logging.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status}")
                        
        except Exception as e:
            logging.error(f"ì™¸ë¶€ ì•Œë¦¼ ì „ì†¡ ì—ëŸ¬: {e}")

    async def broadcast_message(self, message: str):
        """ëª¨ë“  ì—°ê²°ëœ WebSocketì— ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                disconnected.append(connection)
        
        for connection in disconnected:
            self.active_connections.remove(connection)

monitoring = MonitoringService()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    monitoring.active_connections.append(websocket)
    
    try:
        while True:
            await websocket.receive_text()
    except:
        pass
    finally:
        monitoring.active_connections.remove(websocket)

@app.get("/api/stats")
async def get_statistics():
    """ë¡œê·¸ í†µê³„ API"""
    return await monitoring.get_log_statistics()

@app.get("/api/errors")
async def get_recent_errors():
    """ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ API"""
    return await monitoring.get_recent_errors()

@app.get("/api/alerts")
async def get_alerts():
    """ìµœê·¼ ì•Œë¦¼ ëª©ë¡ API"""
    alerts = monitoring.redis_client.lrange("alerts", 0, 99)
    return [json.loads(alert) for alert in alerts]

@app.post("/api/test-alert")
async def create_test_alert():
    """í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ìƒì„±"""
    test_alert = LogAlert(
        timestamp=datetime.now(),
        level="warning",
        service="test-service",
        message="í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì…ë‹ˆë‹¤",
        environment="development",
        alert_type="test"
    )
    await monitoring.send_alert(test_alert)
    return {"message": "í…ŒìŠ¤íŠ¸ ì•Œë¦¼ ìƒì„±ë¨"}

async def monitoring_loop():
    """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
    while True:
        try:
            alerts = await monitoring.detect_anomalies()
            for alert in alerts:
                await monitoring.send_alert(alert)
            
            # í†µê³„ ì •ë³´ë¥¼ WebSocketìœ¼ë¡œ ì „ì†¡
            stats = await monitoring.get_log_statistics()
            if stats and monitoring.active_connections:
                message = {
                    "type": "stats",
                    "data": stats
                }
                await monitoring.broadcast_message(json.dumps(message))
                
        except Exception as e:
            logging.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì—ëŸ¬: {e}")
        
        await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì‹¤í–‰

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ì‹œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘"""
    asyncio.create_task(monitoring_loop())

if __name__ == "__main__":
    import uvicorn
    logging.basicConfig(level=logging.INFO)
    uvicorn.run(app, host="0.0.0.0", port=8080)